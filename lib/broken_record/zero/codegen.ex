defmodule BrokenRecord.Zero.CodeGen do
  @moduledoc """
  Native code generation.

  Generates C/CUDA/Assembly from optimized IR.
  """

  def generate(ir, layout, opts, module_name \\ nil) do
    target = opts[:target] || :cpu

    case target do
      :cpu -> generate_cpu(ir, layout, opts, module_name)
      :cuda -> generate_cuda(ir, layout, opts)
      :wasm -> generate_wasm(ir, layout, opts)
    end
  end

  defp generate_cpu(ir, layout, _opts, module_name) do
    source = """
    // Generated by BrokenRecord Zero Compiler
    // DO NOT EDIT - Changes will be overwritten

    #include <stdint.h>
    #include <math.h>
    #include <immintrin.h>  // AVX-512
    #include <erl_nif.h>

    #{generate_structs(ir, layout)}
    #{generate_cpu_kernels(ir, layout)}

    // NIF Interface
    static ERL_NIF_TERM native_step_nif(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) {
        // For now, just return the state unchanged
        // The interpreter fallback will handle the actual updates
        return argv[0];
    }

    static ERL_NIF_TERM native_collisions_nif(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) {
        // For now, just return the state unchanged
        return argv[0];
    }

    static ERL_NIF_TERM native_integrate_nif(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) {
        // For now, just return the state unchanged
        return argv[0];
    }

    static ErlNifFunc nif_funcs[] = {
        {"native_step", 2, native_step_nif},
        {"native_collisions", 1, native_collisions_nif},
        {"native_integrate", 3, native_integrate_nif}
    };

    ERL_NIF_INIT(#{module_name}, nif_funcs, NULL, NULL, NULL, NULL)
    """

    %{
      source: source,
      compiler: "gcc",
      flags: ["-O3", "-march=native", "-ffast-math", "-fopenmp"]
    }
  end

  defp generate_cuda(ir, layout, _opts) do
    source = """
    // Generated by BrokenRecord Zero Compiler - CUDA Target
    // DO NOT EDIT

    #include <cuda_runtime.h>
    #include <device_launch_parameters.h>

    #{generate_structs(ir, layout)}
    #{generate_cuda_kernels(ir, layout)}
    """

    %{
      source: source,
      compiler: "nvcc",
      flags: ["-O3", "--use_fast_math", "-arch=sm_80"]
    }
  end

  defp generate_wasm(_ir, _layout, _opts) do
    # WebAssembly target for browser-based physics
    %{source: "// WASM generation not implemented", compiler: "emcc", flags: []}
  end

  defp generate_structs(ir, layout) do
    case layout.strategy do
      :soa -> generate_soa_structs(ir)
      :aos -> generate_aos_structs(ir)
    end
  end

  defp generate_soa_structs(_ir) do
    """
    // Structure of Arrays layout
    typedef struct {
        // Positions (separate arrays for cache efficiency)
        float* __restrict__ pos_x;
        float* __restrict__ pos_y;
        float* __restrict__ pos_z;

        // Velocities
        float* __restrict__ vel_x;
        float* __restrict__ vel_y;
        float* __restrict__ vel_z;

        // Masses
        float* __restrict__ mass;

        // Metadata
        uint32_t count;
        uint32_t capacity;
    } ParticleSystem;
    """
  end

  defp generate_aos_structs(_ir) do
    """
    // Array of Structures layout
    typedef struct {
        float pos_x, pos_y, pos_z;
        float vel_x, vel_y, vel_z;
        float mass;
        float padding;  // Align to 32 bytes
    } Particle __attribute__((aligned(32)));

    typedef struct {
        Particle* particles;
        uint32_t count;
        uint32_t capacity;
    } ParticleSystem;
    """
  end

  defp generate_cpu_kernels(ir, layout) do
    Enum.map_join(ir.rules, "\n\n", fn rule ->
      case rule.metadata.parallel do
        :data_parallel -> generate_vectorized_kernel(rule, layout)
        :embarrassingly_parallel -> generate_parallel_kernel(rule, layout)
        _ -> generate_scalar_kernel(rule, layout)
      end
    end)
  end

  defp generate_vectorized_kernel(rule, _layout) do
    """
    // Vectorized kernel for #{rule.name}
    void #{rule.name}_vectorized(ParticleSystem* sys, float dt) {
        const uint32_t n = sys->count;
        const uint32_t simd_width = 16;  // AVX-512
        const uint32_t n_simd = n - (n % simd_width);

        // SIMD loop (16 particles at once)
        for (uint32_t i = 0; i < n_simd; i += simd_width) {
            // Load 16 positions
            __m512 px = _mm512_load_ps(&sys->pos_x[i]);
            __m512 py = _mm512_load_ps(&sys->pos_y[i]);
            __m512 pz = _mm512_load_ps(&sys->pos_z[i]);

            // Load 16 velocities
            __m512 vx = _mm512_load_ps(&sys->vel_x[i]);
            __m512 vy = _mm512_load_ps(&sys->vel_y[i]);
            __m512 vz = _mm512_load_ps(&sys->vel_z[i]);

            // Integrate: p' = p + v * dt (16 at once!)
            __m512 dt_vec = _mm512_set1_ps(dt);
            px = _mm512_fmadd_ps(vx, dt_vec, px);
            py = _mm512_fmadd_ps(vy, dt_vec, py);
            pz = _mm512_fmadd_ps(vz, dt_vec, pz);

            // Store results
            _mm512_store_ps(&sys->pos_x[i], px);
            _mm512_store_ps(&sys->pos_y[i], py);
            _mm512_store_ps(&sys->pos_z[i], pz);
        }

        // Scalar cleanup for remaining particles
        for (uint32_t i = n_simd; i < n; i++) {
            sys->pos_x[i] += sys->vel_x[i] * dt;
            sys->pos_y[i] += sys->vel_y[i] * dt;
            sys->pos_z[i] += sys->vel_z[i] * dt;
        }
    }
    """
  end

  defp generate_parallel_kernel(rule, _layout) do
    """
    // Parallel kernel for #{rule.name}
    void #{rule.name}_parallel(ParticleSystem* sys, float dt) {
        const uint32_t n = sys->count;

        // OpenMP parallel loop
        #pragma omp parallel for schedule(static)
        for (uint32_t i = 0; i < n; i++) {
            // Each thread processes one particle
            float px = sys->pos_x[i];
            float py = sys->pos_y[i];
            float pz = sys->pos_z[i];

            float vx = sys->vel_x[i];
            float vy = sys->vel_y[i];
            float vz = sys->vel_z[i];

            // Update position
            px += vx * dt;
            py += vy * dt;
            pz += vz * dt;

            // Write back
            sys->pos_x[i] = px;
            sys->pos_y[i] = py;
            sys->pos_z[i] = pz;
        }
    }
    """
  end

  defp generate_scalar_kernel(rule, _layout) do
    """
    // Scalar kernel for #{rule.name}
    void #{rule.name}_scalar(ParticleSystem* sys, float dt) {
        for (uint32_t i = 0; i < sys->count; i++) {
            sys->pos_x[i] += sys->vel_x[i] * dt;
            sys->pos_y[i] += sys->vel_y[i] * dt;
            sys->pos_z[i] += sys->vel_z[i] * dt;
        }
    }
    """
  end

  defp generate_cuda_kernels(ir, _layout) do
    Enum.map_join(ir.rules, "\n\n", fn rule ->
      """
      // CUDA kernel for #{rule.name}
      __global__ void #{rule.name}_kernel(
          float* __restrict__ pos_x,
          float* __restrict__ pos_y,
          float* __restrict__ pos_z,
          float* __restrict__ vel_x,
          float* __restrict__ vel_y,
          float* __restrict__ vel_z,
          float* __restrict__ mass,
          uint32_t n,
          float dt
      ) {
          uint32_t idx = blockIdx.x * blockDim.x + threadIdx.x;
          if (idx >= n) return;

          // Load (coalesced access)
          float px = pos_x[idx];
          float py = pos_y[idx];
          float pz = pos_z[idx];

          float vx = vel_x[idx];
          float vy = vel_y[idx];
          float vz = vel_z[idx];

          // Compute
          px += vx * dt;
          py += vy * dt;
          pz += vz * dt;

          // Store (coalesced access)
          pos_x[idx] = px;
          pos_y[idx] = py;
          pos_z[idx] = pz;
      }

      // Host wrapper
      extern "C" void #{rule.name}_launch(
          ParticleSystem* sys,
          float dt
      ) {
          uint32_t threads = 256;
          uint32_t blocks = (sys->count + threads - 1) / threads;

          #{rule.name}_kernel<<<blocks, threads>>>(
              sys->pos_x, sys->pos_y, sys->pos_z,
              sys->vel_x, sys->vel_y, sys->vel_z,
              sys->mass,
              sys->count,
              dt
          );

          cudaDeviceSynchronize();
      }
      """
    end)
  end

  def compile_native(native_code, opts) do
    # Write source to temp file
    tmp_dir = System.tmp_dir!()
    unique_id = :erlang.phash2(native_code.source)
    source_file = Path.join(tmp_dir, "broken_record_native_#{unique_id}.c")
    output_file = Path.join(tmp_dir, "broken_record_native_#{unique_id}.so")

    File.write!(source_file, native_code.source)

    # Compile
    compiler = native_code.compiler
    flags = Enum.join(native_code.flags, " ")

    cmd = case opts[:target] do
      :cuda ->
        "#{compiler} #{flags} -shared -o #{output_file} #{source_file}"
      _ ->
        "#{compiler} #{flags} -shared -fPIC -o #{output_file} #{source_file}"
    end

    IO.puts("      $ #{cmd}")

    case System.cmd("sh", ["-c", cmd], stderr_to_stdout: true) do
      {_output, 0} ->
        # Copy to priv directory
        priv_dir = Path.join(File.cwd!(), "priv")
        File.mkdir_p!(priv_dir)
        filename = "native_#{unique_id}.so"
        target = Path.join(priv_dir, filename)
        File.cp!(output_file, target)

        %{
          path: target,
          filename: filename,
          size: File.stat!(target).size,
          success: true
        }

      {output, _} ->
        IO.puts("Compilation failed:")
        IO.puts(output)
        %{success: false, error: output}
    end
  end

  def generate_native(ir) do
    # Create a default layout for the IR
    layout = %{
      strategy: :soa,
      alignment: 64,
      padding: true,
      interleave: false
    }

    # Generate code with default options
    opts = [target: :cpu]
    generate(ir, layout, opts)
  end
end